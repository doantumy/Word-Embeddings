[The work will be done by end of June 2018]

# Context 
The main purpose of this work is to understand about the word embedding (part of language modelling techniques for NLP), its applications, and how to build a model that can be used for text processing with the help of some related Deep Learning packages/libraries for NLP. 

# Expected results
## Theoretical:
* What is word embedding for text, RNN
* Algorithms used for learning word embedding from text data (eg. fasttext, glove,  word2vec developed byÂ Mikolov et al that is used for learning vector representations of words)
* Some insights about Deep learning packages (eg. TensorFlow)
## Practical: 
Apply word embedding in deep learning (Python) (dataset can be chosen from Kaggle)

# References

1. Y. Bengio, R. Ducharme, Vincent, P., and C. Jauvin. 2003. A neural probabilistic language model. Journal of Machine Learning Reseach.
2. Tomas Mikolov, Wen-tau Yih and Geoffrey Zweig. Linguistic Regularities in Continuous Space Word Representations. In Proceedings of NAACL HLT, 2013.
3. Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781 (2013).
4. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado and Jeffrey Dean. Distributed Representations of Words and Phrases and their Compositionality. 	arXiv:1310.4546 (2013).
5. Amit Mandelbaum, Adi Shalev. Word Embeddings and Their Use In Sentence Classification Tasks. arXiv:1610.08229v1(2017).
6. Jeffrey Pennington , Richard Socher , Christopher D. Manning. Glove: Global vectors for word representation. In EMNLP (2014).
7. Armand Joulin, Edouard Grave, Piotr Bojanowski, Tomas Mikolov. Bag of Tricks for Efficient Text Classification. arXiv:1607.01759 (2016)
8. Piotr Bojanowski, Edouard Grave, Armand Joulin, Tomas Mikolov. Enriching Word Vectors with Subword Information. arXiv:1607.04606 (2017)
9. Piotr Bojanowski, Edouard Grave, Armand Joulin, Tomas Mikolov. Facebook Research on fastText\\\texttt{\url{https://research.fb.com/fasttext/}}

