# Context 
The main purpose of this work is to understand about the word embedding (part of language modelling techniques for NLP), its applications, and how to build a model that can be used for text processing with the help of some related Deep Learning packages/libraries for NLP. 

# Expected results
## Theoretical:
* What is word embedding for text, RNN
* Algorithms used for learning word embedding from text data (eg. fasttext, glove,  word2vec developed byÂ Mikolov et al that is used for learning vector representations of words)
* Some insights about Deep learning packages (eg. TensorFlow)
## Practical: 
Apply word embedding in deep learning (Python) (dataset can be chosen from Kaggle)

# References

1.	Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. ICLR Workshop, 2013. https://arxiv.org/pdf/1301.3781.pdf
2.	http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/
3.	CS224n: Natural Language Processing with Deep Learning https://www.datascience.com/resources/notebooks/word-embeddings-in-python http://web.stanford.edu/class/cs224n/syllabus.html
4.	https://www.packtpub.com/application-development/python-text-processing-nltk-20-cookbook
5.	Natural Language Processing with Python http://www.nltk.org/book/

