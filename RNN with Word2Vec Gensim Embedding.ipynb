{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with RNN and `word2vec` Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we are going to build RNN model with pre-trained embedding `word2vec`. Let's first load some required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import nltk, re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from datetime import datetime\n",
    "from gensim.models import *\n",
    "import logging\n",
    "import time\n",
    "from rnn_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same with our previous RNN model and `GloVe`, we will load data and clean it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./data/labeledTrainData.tsv\", delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning data\n",
    "\n",
    "After finishing loading the dataset, we will now define a function <span style=\"color:blue; font-family:Courier\"> format_train_review</span> to clean up our data. This function will cut the review longer than our maximal length and looking up index for each word in our `word_list`. Padding will be done after we are done cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_train_review(train_review, max_seq_len, word_list):\n",
    "    '''\n",
    "    Converting review into array of index corresponding to its position in the vocabulary\n",
    "    \n",
    "    Arguments:\n",
    "        train_review: review data\n",
    "        max_seq_len: maximal sequence length\n",
    "        word_list: words array containing vocabularies from the learned embedding\n",
    "    \n",
    "    Return:\n",
    "        review array with the same max_seq_len containing index for each word\n",
    "    '''\n",
    "    count = 0\n",
    "    review_clean = clean_sentence(train_review)\n",
    "    review_split = review_clean.split()\n",
    "    if len(review_split) > max_seq_len:\n",
    "        review_max_len = review_split[-max_seq_len:]\n",
    "    else: \n",
    "        review_max_len = review_split\n",
    "\n",
    "    len_rev = len(review_max_len)\n",
    "    temp_rvw = np.zeros(len_rev, dtype = 'int32')\n",
    "    for word in review_max_len:\n",
    "        try:\n",
    "            temp_rvw[count] = word_list.index(word)\n",
    "        except ValueError:\n",
    "            temp_rvw[count] = word_list.index('unk') # if not found, values will be zero\n",
    "        count += 1\n",
    "    \n",
    "    return temp_rvw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load our saved `word_list` and `word_vector` to prepare for the clean up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load_word_list = np.load('./data/word_list_gensim_w2v.npy')\n",
    "load_word_vector = np.load('./data/word_vector_gensim_w2v.npy')\n",
    "load_word_list = load_word_list.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will loop through all reviews in our dataset and call the function <span style=\"color:blue; font-family:Courier\"> format_train_review</span> to clean up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time: 3.0 minutes.\n"
     ]
    }
   ],
   "source": [
    "max_seq_len = 230\n",
    "train_seq = []\n",
    "tic = time.time()\n",
    "for review in train.review:\n",
    "    f = format_train_review(train_review = review, max_seq_len = max_seq_len, word_list = load_word_list)\n",
    "    train_seq.append(f)\n",
    "time = np.round((time.time() - tic)/60)\n",
    "print(\"Processing time: {} minutes.\".format(time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of review after formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   10,    16,     6,   366,     4,  2068,    37,  1204,  2183,\n",
       "        1337,     1,  1301,    26,     2,   106,   371,     1, 24801,\n",
       "         383,    28,     3,   726,  2710,    40,  3156,     5,   816,\n",
       "       10481,    10,    16,     6,     3,   978,    68,   699,    71,\n",
       "          49,    41,     7,     1,   679], dtype=int32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_seq[9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reviews truncating\n",
    "\n",
    "When we clean up data as above, we only truncate reviews that are longer than our maximal sequence length but we dind't do padding for shorter reviews. In order to make all of them into the same length, we have to apply padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of review 9th: 230\n",
      "\n",
      "Total number of reviews: 25000\n"
     ]
    }
   ],
   "source": [
    "train_pad = pad_sequences(train_seq, maxlen = max_seq_len, padding='pre')\n",
    "print(\"Length of review 9th: {}\\n\".format(len(train_pad[9])))\n",
    "print(\"Total number of reviews: {}\".format(len(train_pad)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in our vocabulary: 24802\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of words in our vocabulary: {}\".format(len(load_word_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check for indices of random review using our `load_word_list`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'movie', 'is', 'full', 'of', 'references', 'like', 'mad', 'max', 'ii', 'the', 'wild', 'one', 'and', 'many', 'others', 'the', 'ladybugs', 'face', 'its', 'a', 'clear', 'reference', 'or', 'tribute', 'to', 'peter', 'lorre', 'this', 'movie', 'is', 'a', 'masterpiece', 'well', 'talk', 'much', 'more', 'about', 'in', 'the', 'future']\n"
     ]
    }
   ],
   "source": [
    "print(clean_sentence(train.review[9]).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of word 'this':       10\n",
      "\n",
      "Index of word 'movie':      16\n",
      "\n",
      "Index of word 'is':         6\n",
      "\n",
      "Index of word 'full':       366\n",
      "\n",
      "Index of word 'of':         4\n",
      "\n",
      "Index of word 'references': 2068\n",
      "\n",
      "Index of word 'like':       37\n",
      "\n",
      "Index of word 'mad':        1204\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Index of word 'this':       {}\\n\".format(load_word_list.index('this')))\n",
    "print(\"Index of word 'movie':      {}\\n\".format(load_word_list.index('movie')))\n",
    "print(\"Index of word 'is':         {}\\n\".format(load_word_list.index('is')))\n",
    "print(\"Index of word 'full':       {}\\n\".format(load_word_list.index('full')))\n",
    "print(\"Index of word 'of':         {}\\n\".format(load_word_list.index('of')))\n",
    "print(\"Index of word 'references': {}\\n\".format(load_word_list.index('references')))\n",
    "print(\"Index of word 'like':       {}\\n\".format(load_word_list.index('like')))\n",
    "print(\"Index of word 'mad':        {}\\n\".format(load_word_list.index('mad')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a sample review in the form of index, we can see that each word is now converted into index corresponding to its position in the `load_word_list`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "          10,    16,     6,   366,     4,  2068,    37,  1204,  2183,\n",
       "        1337,     1,  1301,    26,     2,   106,   371,     1, 24801,\n",
       "         383,    28,     3,   726,  2710,    40,  3156,     5,   816,\n",
       "       10481,    10,    16,     6,     3,   978,    68,   699,    71,\n",
       "          49,    41,     7,     1,   679], dtype=int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pad[9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting data\n",
    "\n",
    "Original training clean data will be splitted into training set, validation set and test set because our test data doesn't have labels. First, I will take around 2,000 examples as our test data and the rest will be used as training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_test: (1984, 230)\n",
      "Shape of y_test: (1984,)\n"
     ]
    }
   ],
   "source": [
    "x_test = train_pad[0:1984]\n",
    "y_test = train.sentiment[0:1984]\n",
    "print(\"Shape of x_test: \"+ str(x_test.shape))\n",
    "print(\"Shape of y_test: \"+ str(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of x_train: (19563, 230)\n",
      "Length of y_train: (19563,) \n",
      "\n",
      "Shape of x_valid: (3453, 230)\n",
      "Shape of y_valid: (3453,)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(train_pad[1984:], \n",
    "                                                    train.sentiment[1984:], \n",
    "                                                    test_size = 0.15, \n",
    "                                                    random_state = 789)\n",
    "\n",
    "print(\"Length of x_train: \"+ str(x_train.shape))\n",
    "print(\"Length of y_train: \"+ str(y_train.shape) +\" \\n\")\n",
    "print(\"Shape of x_valid: \"+ str(x_valid.shape))\n",
    "print(\"Shape of y_valid: \"+ str(y_valid.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batching training data\n",
    "\n",
    "Spliting them into batches. Function <span style=\"color:blue; font-family:Courier\">mini_batch</span> is defined in our file `rnn_utils` because they are just the same with the function we used in previous RNN model so I copied them into another python file for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train batches: 305\n",
      "\n",
      "Number of validation batches: 53\n",
      "\n",
      "Number of test batches: 31\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "mini_batches_train = mini_batch(x_train, y_train, batch_size)\n",
    "mini_batches_valid = mini_batch(x_valid, y_valid, batch_size)\n",
    "mini_batches_test = mini_batch(x_test, y_test, batch_size)\n",
    "print(\"Number of train batches: {}\\n\".format(len(mini_batches_train)))\n",
    "print(\"Number of validation batches: {}\\n\".format(len(mini_batches_valid)))\n",
    "print(\"Number of test batches: {}\\n\".format(len(mini_batches_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First x train mini batch: (Shape: (64, 230)) \n",
      "[[    0     0     0 ...,   153 13430  1218]\n",
      " [  474     2    21 ...,     3  5809   561]\n",
      " [  524    10   208 ...,   473    25   669]\n",
      " ..., \n",
      " [    0     0     0 ...,   675    11  2388]\n",
      " [    0     0     0 ...,     2    64    89]\n",
      " [    0     0     0 ...,  5129  2051 22874]]\n",
      "\n",
      "First y train mini batch: (Shape: (64,)) \n",
      "7136     0\n",
      "8636     0\n",
      "13793    0\n",
      "17384    0\n",
      "16192    1\n",
      "12175    1\n",
      "7585     1\n",
      "3196     0\n",
      "23543    1\n",
      "22862    0\n",
      "17241    1\n",
      "17189    1\n",
      "20505    1\n",
      "17857    0\n",
      "4012     1\n",
      "9368     1\n",
      "21231    1\n",
      "22919    0\n",
      "22428    1\n",
      "2913     1\n",
      "5762     1\n",
      "3893     1\n",
      "11735    0\n",
      "2198     0\n",
      "3293     1\n",
      "15584    0\n",
      "11576    1\n",
      "14954    0\n",
      "10586    0\n",
      "9157     0\n",
      "        ..\n",
      "11532    0\n",
      "12445    1\n",
      "13802    1\n",
      "20905    1\n",
      "4659     1\n",
      "15565    0\n",
      "20667    1\n",
      "6950     0\n",
      "10922    1\n",
      "6686     1\n",
      "4427     1\n",
      "10779    1\n",
      "22493    0\n",
      "4315     0\n",
      "2726     0\n",
      "22101    1\n",
      "4628     1\n",
      "7226     1\n",
      "6263     0\n",
      "19972    0\n",
      "16865    1\n",
      "11839    1\n",
      "12638    0\n",
      "16780    1\n",
      "19065    0\n",
      "19045    0\n",
      "22109    0\n",
      "5206     0\n",
      "20663    1\n",
      "11456    0\n",
      "Name: sentiment, Length: 64, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"First x train mini batch: (Shape: {1}) \\n{0}\\n\".format(mini_batches_train[0][0], mini_batches_train[0][0].shape))\n",
    "print(\"First y train mini batch: (Shape: {1}) \\n{0}\".format(mini_batches_train[0][1], mini_batches_train[0][1].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building RNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow Graph & Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values we used here are the same with our RNN with `GloVe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_words = len(load_word_list)\n",
    "embed_size = 50\n",
    "num_layers = 1\n",
    "lstm_size = 64\n",
    "n_epochs = 80\n",
    "prob = 0.5\n",
    "seq_len = 230\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build TensorGraph\n",
    "\n",
    "The only difference here is that we will use `load_word_vector` as our embedding vector which created as a result of learning embedding with `Gensim word2vec` before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    # Define Placeholders\n",
    "    tf_x = tf.placeholder(dtype = tf.int32, shape = (batch_size, seq_len), name = \"tf_x\")\n",
    "    tf_y = tf.placeholder(dtype = tf.float32, shape = (batch_size), name = \"tf_y\")\n",
    "    tf_keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "    # Create Embedded layer\n",
    "    embedding = tf.nn.embedding_lookup(tf.cast(load_word_vector, tf.float32), tf_x, name='embedding')\n",
    "\n",
    "    # Define LSTM cells\n",
    "    drop_prob = tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.BasicLSTMCell(lstm_size), output_keep_prob=tf_keep_prob)\n",
    "    lstm_cells = tf.contrib.rnn.MultiRNNCell([drop_prob] * num_layers)\n",
    "\n",
    "    # Set Initial state\n",
    "    init_state = lstm_cells.zero_state(batch_size, tf.float32)\n",
    "    lstm_outputs, final_state = tf.nn.dynamic_rnn(lstm_cells, embedding, initial_state=init_state)\n",
    "    \n",
    "    logits = tf.squeeze(tf.layers.dense(inputs=lstm_outputs[:,-1], units = 1, activation=None, name = 'logits'))\n",
    "    y_prob = tf.nn.sigmoid(logits, name = 'probabilities')\n",
    "    \n",
    "    predictions = {'probabilities': y_prob,\n",
    "                   'labels' : tf.cast(tf.round(y_prob), tf.int32,name='labels')}\n",
    "    # Cost\n",
    "    cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels = tf_y))\n",
    "    tf.summary.scalar('cost', cost)\n",
    "    \n",
    "    # Optimizer\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    starter_learning_rate = 0.1\n",
    "    learning_rt = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                           1000, 0.96, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rt)\n",
    "    train_op = optimizer.minimize(cost, name = 'train_op')\n",
    "    \n",
    "    # Accuracy\n",
    "    correct_pred = tf.equal(tf.round(y_prob), tf_y)\n",
    "    acc = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    tf.summary.scalar('accuracy', acc)\n",
    "    \n",
    "    merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train RNN model\n",
    "\n",
    "It's time to train the model. Here we will build single layer RNN with 64 LSTM hidden units. The output activation function will be sigmoid function as we only need `1` (for positive review) or `0` (negative review).\n",
    "\n",
    "Changes in the accuracy and loss can be monitored via TensorBoard. Later when the training is done, I will display the screenshots of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/80 | Train loss: 0.4844 | Train accuracy: 0.7812\n",
      "Epoch: 1/80 | Validation loss: 0.4805 | Validation accuracy: 0.7656\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_1.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 179\n",
      "Epoch: 2/80 | Train loss: 0.4664 | Train accuracy: 0.7812\n",
      "Epoch: 2/80 | Validation loss: 0.4742 | Validation accuracy: 0.7656\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_2.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 190\n",
      "Epoch: 3/80 | Train loss: 0.3852 | Train accuracy: 0.8438\n",
      "Epoch: 3/80 | Validation loss: 0.3588 | Validation accuracy: 0.8438\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_3.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 183\n",
      "Epoch: 4/80 | Train loss: 0.3879 | Train accuracy: 0.8281\n",
      "Epoch: 4/80 | Validation loss: 0.3939 | Validation accuracy: 0.7969\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_4.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 171\n",
      "Epoch: 5/80 | Train loss: 0.3664 | Train accuracy: 0.8438\n",
      "Epoch: 5/80 | Validation loss: 0.4330 | Validation accuracy: 0.7812\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_5.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 170\n",
      "Epoch: 6/80 | Train loss: 0.3790 | Train accuracy: 0.8438\n",
      "Epoch: 6/80 | Validation loss: 0.3737 | Validation accuracy: 0.8438\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_6.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 4678\n",
      "Epoch: 7/80 | Train loss: 0.3534 | Train accuracy: 0.8750\n",
      "Epoch: 7/80 | Validation loss: 0.3485 | Validation accuracy: 0.8438\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_7.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 193\n",
      "Epoch: 8/80 | Train loss: 0.3449 | Train accuracy: 0.8750\n",
      "Epoch: 8/80 | Validation loss: 0.3237 | Validation accuracy: 0.8906\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_8.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 201\n",
      "Epoch: 9/80 | Train loss: 0.3718 | Train accuracy: 0.8594\n",
      "Epoch: 9/80 | Validation loss: 0.2695 | Validation accuracy: 0.9062\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_9.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 186\n",
      "Epoch: 10/80 | Train loss: 0.3139 | Train accuracy: 0.8906\n",
      "Epoch: 10/80 | Validation loss: 0.2841 | Validation accuracy: 0.8906\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_10.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 179\n",
      "Epoch: 11/80 | Train loss: 0.3088 | Train accuracy: 0.8906\n",
      "Epoch: 11/80 | Validation loss: 0.2982 | Validation accuracy: 0.9219\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_11.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 192\n",
      "Epoch: 12/80 | Train loss: 0.2931 | Train accuracy: 0.8906\n",
      "Epoch: 12/80 | Validation loss: 0.2987 | Validation accuracy: 0.8594\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_12.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 181\n",
      "Epoch: 13/80 | Train loss: 0.2713 | Train accuracy: 0.8281\n",
      "Epoch: 13/80 | Validation loss: 0.3794 | Validation accuracy: 0.8438\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_13.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 185\n",
      "Epoch: 14/80 | Train loss: 0.2781 | Train accuracy: 0.8906\n",
      "Epoch: 14/80 | Validation loss: 0.2843 | Validation accuracy: 0.8750\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_14.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 211\n",
      "Epoch: 15/80 | Train loss: 0.2609 | Train accuracy: 0.8906\n",
      "Epoch: 15/80 | Validation loss: 0.2774 | Validation accuracy: 0.8750\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_15.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 246\n",
      "Epoch: 16/80 | Train loss: 0.3260 | Train accuracy: 0.8750\n",
      "Epoch: 16/80 | Validation loss: 0.3189 | Validation accuracy: 0.8906\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_16.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 205\n",
      "Epoch: 17/80 | Train loss: 0.2750 | Train accuracy: 0.8906\n",
      "Epoch: 17/80 | Validation loss: 0.2630 | Validation accuracy: 0.9219\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_17.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 215\n",
      "Epoch: 18/80 | Train loss: 0.2882 | Train accuracy: 0.8750\n",
      "Epoch: 18/80 | Validation loss: 0.3100 | Validation accuracy: 0.8594\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_18.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 210\n",
      "Epoch: 19/80 | Train loss: 0.2859 | Train accuracy: 0.8594\n",
      "Epoch: 19/80 | Validation loss: 0.2864 | Validation accuracy: 0.8594\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_19.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 222\n",
      "Epoch: 20/80 | Train loss: 0.4211 | Train accuracy: 0.7656\n",
      "Epoch: 20/80 | Validation loss: 0.3953 | Validation accuracy: 0.8438\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_20.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 196\n",
      "Epoch: 21/80 | Train loss: 0.2200 | Train accuracy: 0.9219\n",
      "Epoch: 21/80 | Validation loss: 0.2571 | Validation accuracy: 0.9375\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_21.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 208\n",
      "Epoch: 22/80 | Train loss: 0.2168 | Train accuracy: 0.9062\n",
      "Epoch: 22/80 | Validation loss: 0.3522 | Validation accuracy: 0.8281\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_22.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 205\n",
      "Epoch: 23/80 | Train loss: 0.2429 | Train accuracy: 0.9062\n",
      "Epoch: 23/80 | Validation loss: 0.3550 | Validation accuracy: 0.8281\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_23.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 219\n",
      "Epoch: 24/80 | Train loss: 0.1646 | Train accuracy: 0.9219\n",
      "Epoch: 24/80 | Validation loss: 0.3574 | Validation accuracy: 0.8750\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_24.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 210\n",
      "Epoch: 25/80 | Train loss: 0.2292 | Train accuracy: 0.9062\n",
      "Epoch: 25/80 | Validation loss: 0.2578 | Validation accuracy: 0.8906\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_25.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 216\n",
      "Epoch: 26/80 | Train loss: 0.2438 | Train accuracy: 0.8906\n",
      "Epoch: 26/80 | Validation loss: 0.2502 | Validation accuracy: 0.9062\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_26.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 235\n",
      "Epoch: 27/80 | Train loss: 0.1823 | Train accuracy: 0.9219\n",
      "Epoch: 27/80 | Validation loss: 0.3058 | Validation accuracy: 0.8594\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_27.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 203\n",
      "Epoch: 28/80 | Train loss: 0.1649 | Train accuracy: 0.9531\n",
      "Epoch: 28/80 | Validation loss: 0.2497 | Validation accuracy: 0.9062\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_28.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 213\n",
      "Epoch: 29/80 | Train loss: 0.1744 | Train accuracy: 0.9688\n",
      "Epoch: 29/80 | Validation loss: 0.2291 | Validation accuracy: 0.9062\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_29.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 227\n",
      "Epoch: 30/80 | Train loss: 0.1621 | Train accuracy: 0.9375\n",
      "Epoch: 30/80 | Validation loss: 0.2495 | Validation accuracy: 0.9062\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_30.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 230\n",
      "Epoch: 31/80 | Train loss: 0.1814 | Train accuracy: 0.9531\n",
      "Epoch: 31/80 | Validation loss: 0.3071 | Validation accuracy: 0.8750\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_31.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 240\n",
      "Epoch: 32/80 | Train loss: 0.1954 | Train accuracy: 0.9219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32/80 | Validation loss: 0.2665 | Validation accuracy: 0.8906\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_32.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 232\n",
      "Epoch: 33/80 | Train loss: 0.1605 | Train accuracy: 0.9219\n",
      "Epoch: 33/80 | Validation loss: 0.2272 | Validation accuracy: 0.9219\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_33.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 219\n",
      "Epoch: 34/80 | Train loss: 0.1079 | Train accuracy: 0.9688\n",
      "Epoch: 34/80 | Validation loss: 0.3111 | Validation accuracy: 0.8750\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_34.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 226\n",
      "Epoch: 35/80 | Train loss: 0.1905 | Train accuracy: 0.9219\n",
      "Epoch: 35/80 | Validation loss: 0.2512 | Validation accuracy: 0.8750\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_35.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 266\n",
      "Epoch: 36/80 | Train loss: 0.1377 | Train accuracy: 0.9375\n",
      "Epoch: 36/80 | Validation loss: 0.2776 | Validation accuracy: 0.8906\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_36.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 196\n",
      "Epoch: 37/80 | Train loss: 0.1331 | Train accuracy: 0.9375\n",
      "Epoch: 37/80 | Validation loss: 0.2561 | Validation accuracy: 0.9062\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_37.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 200\n",
      "Epoch: 38/80 | Train loss: 0.1167 | Train accuracy: 0.9375\n",
      "Epoch: 38/80 | Validation loss: 0.2637 | Validation accuracy: 0.9062\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_38.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 195\n",
      "Epoch: 39/80 | Train loss: 0.0804 | Train accuracy: 0.9688\n",
      "Epoch: 39/80 | Validation loss: 0.2795 | Validation accuracy: 0.8906\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_39.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 194\n",
      "Epoch: 40/80 | Train loss: 0.1557 | Train accuracy: 0.9531\n",
      "Epoch: 40/80 | Validation loss: 0.3424 | Validation accuracy: 0.8750\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_40.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 227\n",
      "Epoch: 41/80 | Train loss: 0.1244 | Train accuracy: 0.9531\n",
      "Epoch: 41/80 | Validation loss: 0.2385 | Validation accuracy: 0.9219\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_41.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 219\n",
      "Epoch: 42/80 | Train loss: 0.1189 | Train accuracy: 0.9844\n",
      "Epoch: 42/80 | Validation loss: 0.2277 | Validation accuracy: 0.9219\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_42.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 230\n",
      "Epoch: 43/80 | Train loss: 0.1884 | Train accuracy: 0.9062\n",
      "Epoch: 43/80 | Validation loss: 0.2463 | Validation accuracy: 0.9375\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_43.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 242\n",
      "Epoch: 44/80 | Train loss: 0.0756 | Train accuracy: 0.9844\n",
      "Epoch: 44/80 | Validation loss: 0.2220 | Validation accuracy: 0.9375\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_44.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 218\n",
      "Epoch: 45/80 | Train loss: 0.0970 | Train accuracy: 0.9688\n",
      "Epoch: 45/80 | Validation loss: 0.2027 | Validation accuracy: 0.9531\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_45.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 244\n",
      "Epoch: 46/80 | Train loss: 0.1575 | Train accuracy: 0.9219\n",
      "Epoch: 46/80 | Validation loss: 0.1771 | Validation accuracy: 0.9688\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_46.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 230\n",
      "Epoch: 47/80 | Train loss: 0.1118 | Train accuracy: 0.9844\n",
      "Epoch: 47/80 | Validation loss: 0.2175 | Validation accuracy: 0.9375\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_47.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 241\n",
      "Epoch: 48/80 | Train loss: 0.0932 | Train accuracy: 0.9844\n",
      "Epoch: 48/80 | Validation loss: 0.2708 | Validation accuracy: 0.9062\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_48.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 226\n",
      "Epoch: 49/80 | Train loss: 0.1072 | Train accuracy: 0.9688\n",
      "Epoch: 49/80 | Validation loss: 0.2414 | Validation accuracy: 0.9531\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_49.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 218\n",
      "Epoch: 50/80 | Train loss: 0.1451 | Train accuracy: 0.9219\n",
      "Epoch: 50/80 | Validation loss: 0.2762 | Validation accuracy: 0.9219\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_50.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 213\n",
      "Epoch: 51/80 | Train loss: 0.1287 | Train accuracy: 0.9375\n",
      "Epoch: 51/80 | Validation loss: 0.2215 | Validation accuracy: 0.9531\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_51.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 218\n",
      "Epoch: 52/80 | Train loss: 0.1036 | Train accuracy: 0.9688\n",
      "Epoch: 52/80 | Validation loss: 0.2450 | Validation accuracy: 0.9375\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_52.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 218\n",
      "Epoch: 53/80 | Train loss: 0.2572 | Train accuracy: 0.9062\n",
      "Epoch: 53/80 | Validation loss: 0.3287 | Validation accuracy: 0.8594\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_53.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 256\n",
      "Epoch: 54/80 | Train loss: 0.2176 | Train accuracy: 0.9219\n",
      "Epoch: 54/80 | Validation loss: 0.2464 | Validation accuracy: 0.9219\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_54.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 233\n",
      "Epoch: 55/80 | Train loss: 0.1786 | Train accuracy: 0.9531\n",
      "Epoch: 55/80 | Validation loss: 0.2144 | Validation accuracy: 0.9375\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_55.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 228\n",
      "Epoch: 56/80 | Train loss: 0.1798 | Train accuracy: 0.9062\n",
      "Epoch: 56/80 | Validation loss: 0.3437 | Validation accuracy: 0.8750\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_56.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 202\n",
      "Epoch: 57/80 | Train loss: 0.1044 | Train accuracy: 0.9844\n",
      "Epoch: 57/80 | Validation loss: 0.2512 | Validation accuracy: 0.9219\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_57.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 181\n",
      "Epoch: 58/80 | Train loss: 0.0852 | Train accuracy: 0.9844\n",
      "Epoch: 58/80 | Validation loss: 0.2527 | Validation accuracy: 0.9219\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_58.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 177\n",
      "Epoch: 59/80 | Train loss: 0.0924 | Train accuracy: 0.9688\n",
      "Epoch: 59/80 | Validation loss: 0.3411 | Validation accuracy: 0.8906\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_59.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 190\n",
      "Epoch: 60/80 | Train loss: 0.1147 | Train accuracy: 0.9375\n",
      "Epoch: 60/80 | Validation loss: 0.2053 | Validation accuracy: 0.9375\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_60.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 177\n",
      "Epoch: 61/80 | Train loss: 0.1024 | Train accuracy: 0.9688\n",
      "Epoch: 61/80 | Validation loss: 0.2556 | Validation accuracy: 0.9062\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_61.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 183\n",
      "Epoch: 62/80 | Train loss: 0.0928 | Train accuracy: 0.9688\n",
      "Epoch: 62/80 | Validation loss: 0.2893 | Validation accuracy: 0.9062\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_62.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 180\n",
      "Epoch: 63/80 | Train loss: 0.0550 | Train accuracy: 0.9844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 63/80 | Validation loss: 0.2833 | Validation accuracy: 0.9375\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_63.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 174\n",
      "Epoch: 64/80 | Train loss: 0.0585 | Train accuracy: 1.0000\n",
      "Epoch: 64/80 | Validation loss: 0.3403 | Validation accuracy: 0.9062\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_64.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 177\n",
      "Epoch: 65/80 | Train loss: 0.0974 | Train accuracy: 0.9375\n",
      "Epoch: 65/80 | Validation loss: 0.1970 | Validation accuracy: 0.9688\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_65.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 176\n",
      "Epoch: 66/80 | Train loss: 0.0927 | Train accuracy: 0.9688\n",
      "Epoch: 66/80 | Validation loss: 0.3519 | Validation accuracy: 0.9219\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_66.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 175\n",
      "Epoch: 67/80 | Train loss: 0.0544 | Train accuracy: 0.9844\n",
      "Epoch: 67/80 | Validation loss: 0.3251 | Validation accuracy: 0.9062\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_67.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 175\n",
      "Epoch: 68/80 | Train loss: 0.0633 | Train accuracy: 0.9688\n",
      "Epoch: 68/80 | Validation loss: 0.3481 | Validation accuracy: 0.8906\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_68.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 170\n",
      "Epoch: 69/80 | Train loss: 0.1230 | Train accuracy: 0.9688\n",
      "Epoch: 69/80 | Validation loss: 0.2603 | Validation accuracy: 0.9219\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_69.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 178\n",
      "Epoch: 70/80 | Train loss: 0.1193 | Train accuracy: 0.9531\n",
      "Epoch: 70/80 | Validation loss: 0.2921 | Validation accuracy: 0.9062\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_70.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 184\n",
      "Epoch: 71/80 | Train loss: 0.0774 | Train accuracy: 0.9688\n",
      "Epoch: 71/80 | Validation loss: 0.3241 | Validation accuracy: 0.9219\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_71.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 184\n",
      "Epoch: 72/80 | Train loss: 0.2783 | Train accuracy: 0.8750\n",
      "Epoch: 72/80 | Validation loss: 0.3633 | Validation accuracy: 0.8281\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_72.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 173\n",
      "Epoch: 73/80 | Train loss: 0.1975 | Train accuracy: 0.8750\n",
      "Epoch: 73/80 | Validation loss: 0.2028 | Validation accuracy: 0.9375\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_73.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 179\n",
      "Epoch: 74/80 | Train loss: 0.1332 | Train accuracy: 0.9375\n",
      "Epoch: 74/80 | Validation loss: 0.3114 | Validation accuracy: 0.8906\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_74.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 187\n",
      "Epoch: 75/80 | Train loss: 0.1900 | Train accuracy: 0.9375\n",
      "Epoch: 75/80 | Validation loss: 0.2660 | Validation accuracy: 0.9219\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_75.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 177\n",
      "Epoch: 76/80 | Train loss: 0.1337 | Train accuracy: 0.9375\n",
      "Epoch: 76/80 | Validation loss: 0.2951 | Validation accuracy: 0.9375\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_76.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 186\n",
      "Epoch: 77/80 | Train loss: 0.0819 | Train accuracy: 0.9688\n",
      "Epoch: 77/80 | Validation loss: 0.3104 | Validation accuracy: 0.9219\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_77.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 210\n",
      "Epoch: 78/80 | Train loss: 0.0808 | Train accuracy: 0.9844\n",
      "Epoch: 78/80 | Validation loss: 0.2886 | Validation accuracy: 0.9531\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_78.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 211\n",
      "Epoch: 79/80 | Train loss: 0.0638 | Train accuracy: 0.9844\n",
      "Epoch: 79/80 | Validation loss: 0.3399 | Validation accuracy: 0.8750\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_79.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 215\n",
      "Epoch: 80/80 | Train loss: 0.1181 | Train accuracy: 0.9375\n",
      "Epoch: 80/80 | Validation loss: 0.2951 | Validation accuracy: 0.9219\n",
      "INFO:tensorflow:./model/w2vgensim/w2v_review_sentiment_epoch_80.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 225\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph = g) as sess:\n",
    "    saver = tf.train.Saver(max_to_keep=None)\n",
    "    dt = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    logdir = \"tensorboard/\" + dt + \"/\"\n",
    "    writer = tf.summary.FileWriter(logdir, g)\n",
    "    writer_valid = tf.summary.FileWriter('tensorboard/'+ dt +'_valid', g)\n",
    "    writer_train = tf.summary.FileWriter('tensorboard/'+ dt +'_train', g)\n",
    "    iteration = 1\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        tic = datetime.now()\n",
    "        # Running train data\n",
    "        state = sess.run(init_state)\n",
    "        for batch_x_train, batch_y_train in mini_batches_train:\n",
    "            summary, c, _, state, a = sess.run([merged, cost, train_op, final_state, acc],\n",
    "                                     feed_dict = {'tf_x:0' : batch_x_train,\n",
    "                                                 'tf_y:0' : batch_y_train,\n",
    "                                                 init_state : state,\n",
    "                                                 tf_keep_prob : prob})\n",
    "\n",
    "            writer.add_summary(summary, iteration)\n",
    "            iteration +=1\n",
    "        writer_train.add_summary(summary, epoch+1)\n",
    "        print(\"Epoch: {0}/{1} | Train loss: {2:.4f} | Train accuracy: {3:.4f}\".format(epoch+1, \n",
    "                                                                                      n_epochs, \n",
    "                                                                                      c, \n",
    "                                                                                      a))\n",
    "        # Running validation data\n",
    "        valid_state = sess.run(init_state)\n",
    "        for batch_x_valid, batch_y_valid in mini_batches_valid:\n",
    "            summary, c_valid, valid_state, a_valid = sess.run([merged, cost, final_state, acc],\n",
    "                                     feed_dict = {'tf_x:0' : batch_x_valid,\n",
    "                                                 'tf_y:0' : batch_y_valid,\n",
    "                                                 init_state : valid_state,\n",
    "                                                 tf_keep_prob : 1})\n",
    "        writer_valid.add_summary(summary, epoch+1)\n",
    "        print(\"Epoch: {0}/{1} | Validation loss: {2:.4f} | Validation accuracy: {3:.4f}\".format(epoch+1, \n",
    "                                                                                                n_epochs, \n",
    "                                                                                                c_valid, \n",
    "                                                                                                a_valid))\n",
    "        # Save model every epoch\n",
    "        saver.save(sess,\"./model/w2vgensim/w2v_review_sentiment_epoch_{}.ckpt\".format(epoch+1))\n",
    "        \n",
    "        toc = datetime.now()\n",
    "        time = (toc - tic)\n",
    "        print(\"Time: {}\".format(time.seconds))\n",
    "writer.close()\n",
    "writer_train.close()\n",
    "writer_valid.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training accuracy and loss via TensorBoard\n",
    "\n",
    "TensorBoard for Accuracy and the Loss during the training time after 80 epochs. \n",
    "\n",
    "\n",
    "![](./images/acc_w2v.png)\n",
    "\n",
    "\n",
    "![](./images/cost_w2v.png)\n",
    "\n",
    "Below is our graph for detecting over-fitting during training. We will pick model 19 as our last model. However, it may not clearly because of the smoothing value in the tensorgraph that after epoch 19, model began to over-fit trianing data.\n",
    "\n",
    "- <span style=\"color:rgb(70,173,193)\">Blue line </span>: training cost\n",
    "- <span style=\"color:rgb(173,73,190)\">Purple line</span>: validation cost\n",
    "\n",
    "![](./images/cost_trvd_w2v.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with our test dataset\n",
    "\n",
    "Let's use our test set to test the trained model and see what is the accuracy rate. Same with the training, we also create batches for test set and run several iterations through it and keep track of accuracy in every iteration and the average of them.\n",
    "\n",
    "Below is the number of test batches we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mini_batches_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's pick our 19th model and restore it in TensorFlow session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 88.4073\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph = g) as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    accuracy = []\n",
    "    saver.restore(sess, './data/model/w2vgensim/w2v_review_sentiment_epoch_19.ckpt')\n",
    "    test_state = sess.run(init_state)\n",
    "    for batch_x, batch_y in mini_batches_test:\n",
    "        feed = {'tf_x:0': batch_x, \n",
    "            'tf_y:0': batch_y,\n",
    "            'keep_prob:0' : 1, \n",
    "            init_state : test_state}\n",
    "        a, test_state = sess.run([acc, final_state], feed_dict=feed)\n",
    "        accuracy.append(a)\n",
    "    print(\"Overall accuracy: {0:.4f}\".format(np.mean(accuracy)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model with our own review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Same reviews will be used to test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_review_neg = \"Content is very boring and this is a waste of time to see it.\"\n",
    "str_review_pos = \"Movie is about a spy, which is not new subject. Content is very good and this is a great time to see it.\"\n",
    "user_review_neg = format_user_review(train_review = str_review_neg, batch_size = 64,  max_seq_len = 230, word_list = load_word_list)\n",
    "user_review_pos = format_user_review(train_review = str_review_pos, batch_size = 64,  max_seq_len = 230, word_list = load_word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running this code, we have to make sure that our TensorGraph <span style=\"color:red\">was already created</span>. \n",
    "Let's try with negative review first:\n",
    "- `1` is for positive\n",
    "- `0` is for negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a negative review.\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph = g) as sess:\n",
    "    saver.restore(sess, './data/model/w2vgensim/w2v_review_sentiment_epoch_19.ckpt')\n",
    "    own_state = sess.run(init_state)\n",
    "    \n",
    "    feed = {'tf_x:0': user_review_neg,\n",
    "            'keep_prob:0' : 1,\n",
    "            init_state : own_state}\n",
    "    \n",
    "    lbl, own_state = sess.run(['labels:0', final_state], feed_dict=feed)\n",
    "    if lbl[-1] == 0:\n",
    "        print(\"This is a negative review.\")\n",
    "    else:\n",
    "        print(\"This is a positive review.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Below is the result with our positive review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a positive review.\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph = g) as sess:\n",
    "    saver.restore(sess, './data/model/w2vgensim/w2v_review_sentiment_epoch_19.ckpt')\n",
    "    own_state = sess.run(init_state)\n",
    "    \n",
    "    feed = {'tf_x:0': user_review_pos,\n",
    "            'keep_prob:0' : 1,\n",
    "            init_state : own_state}\n",
    "    \n",
    "    lbl, own_state = sess.run(['labels:0', final_state], feed_dict=feed)\n",
    "    if lbl[-1] == 0:\n",
    "        print(\"This is a negative review.\")\n",
    "    else:\n",
    "        print(\"This is a positive review.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the end of our notebook for RNN with `word2vec` embedding vectors. In the next part, we will look at the models for our own trained embedding vectors using `fastText`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
