{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with RNN and `GloVe` Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we are going to build RNN model with pre-trained embedding `GloVe`. Let's first load some required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import nltk, re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from datetime import datetime\n",
    "from gensim.models import *\n",
    "import logging\n",
    "import pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load our training and test data. In the Kaggle competition, we have 3 different datasets namely labeled train data, unlabeled test data, and finally the unlabeled train data which is used for creating the embeddings with method like `word2vec`. \n",
    "\n",
    "But here we are doing the RNN model, hence the unlabeled train and test data won't help. The test dataset which contains no label but only the review details, can't be used in this training process as there are no actual labels for us to compare with our predictions. As result, the training data will be split into train, validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./data/labeledTrainData.tsv\", delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning data\n",
    "\n",
    "After finishing loading the dataset, we will now define a function <span style=\"color:blue; font-family:Courier\"> clean_sentence</span> to clean up our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "special_characters = re.compile(\"[^A-Za-z0-9 ]\")\n",
    "\n",
    "def clean_sentence(data):\n",
    "    '''\n",
    "    Define function to clean up the data by removing HTML tags, stopwords, extra space, non-characters\n",
    "    Arguments:\n",
    "        text: input data for cleaning\n",
    "        \n",
    "    Return:\n",
    "        List of words\n",
    "    '''\n",
    "    data = data.lower().replace(\"<br />\", \" \")\n",
    "    data = data.replace(\"-\", \" \")\n",
    "    data = data.replace(\".\", \". \")\n",
    "    data = re.sub(\"  \", \" \", data)\n",
    "    return re.sub(special_characters, \"\", data.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looping through all reviews in the data and clean them one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish cleaning train data.\n"
     ]
    }
   ],
   "source": [
    "train_clean = []\n",
    "length = []\n",
    "for review in train.review:\n",
    "    s = clean_sentence(review)\n",
    "    train_clean.append(s)\n",
    "    length.append(len(s.split()))\n",
    "print(\"Finish cleaning train data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some cleaned reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i dont know why people think this is such a bad movie its got a pretty good plot some good action and the change of location for harry does not hurt either sure some of its offensive and gratuitous but this is not the only movie like that eastwood is in good form as dirty harry and i liked pat hingle in this movie as the small town cop if you liked dirty harry then you should see this one its a lot better than the dead pool 45\n",
      "\n",
      "this movie could have been very good but comes up way short cheesy special effects and so so acting i could have looked past that if the story wasnt so lousy if there was more of a background story it would have been better the plot centers around an evil druid witch who is linked to this woman who gets migraines the movie drags on and on and never clearly explains anything it just keeps plodding on christopher walken has a part but it is completely senseless as is most of the movie this movie had potential but it looks like some really bad made for tv movie i would avoid this movie \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in train_clean[5:7]:\n",
    "    print(i)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because reviews have different length and to build RNN model, we have to set fixed length for all of the input reviews. Plot below shows us frequency of review length. The average number of words in one review is `230` which can be safe to set as our max review length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of words per review: 232.9248 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAEKCAYAAAA1qaOTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMyuFC4qZmZXCBcXMzErhgmJmZqX4/zK2bm1kBvrxAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1c9c9908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Average number of words per review: {} \\n\".format(sum(length)/len(length)))\n",
    "\n",
    "plt.hist(length, 50, color='green', ec=\"darkgreen\")\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axis([0, 500, 0, 8000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing\n",
    "\n",
    "We will create a tokenized list of words from our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_clean)      # tokenize training data\n",
    "word_index = tokenizer.word_index        # return a dict map words to indices\n",
    "len_word_docs = len(tokenizer.word_docs) # return the len of tokenized dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the information below, there are 88,096 words in our tokenized word list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of word in our tokenizer dict: 16\n",
      "Number of words in our tokenizer: 88096\n"
     ]
    }
   ],
   "source": [
    "print(\"Index of word in our tokenizer dict: {}\".format(tokenizer.word_index['movie']))\n",
    "print(\"Number of words in our tokenizer: {}\".format(len_word_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save `word_index` dictionary for later use by using `pickle` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = open(\"./data/word_index_glove.pkl\", \"wb\")\n",
    "pickle.dump(word_index, output)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's convert the text reviews into index numbers which corresponding with its position in the tokenized word list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting is done.\n"
     ]
    }
   ],
   "source": [
    "train_seq = tokenizer.texts_to_sequences(train_clean)\n",
    "print(\"Converting is done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All review are now convert into indices already, the next step is to create padding and truncate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9th review in words:\n",
      " this movie is full of references like mad max ii the wild one and many others the ladybugs face its a clear reference or tribute to peter lorre this movie is a masterpiece well talk much more about in the future \n",
      "\n",
      "Same review in index:\n",
      "[10, 16, 6, 360, 4, 2076, 37, 1154, 2395, 1537, 1, 1313, 27, 2, 105, 378, 1, 33761, 386, 28, 3, 781, 2887, 40, 3316, 5, 822, 8900, 10, 16, 6, 3, 989, 69, 733, 72, 49, 41, 7, 1, 692]\n"
     ]
    }
   ],
   "source": [
    "print(\"9th review in words:\\n{}\\n\".format(train_clean[9]))\n",
    "print(\"Same review in index:\\n{}\".format(train_seq[9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reviews truncating\n",
    "\n",
    "As usual, reviews in the dataset will come at different length, therefore, we have to define our suitable sequence length. As mentioned earlier, we will set max review length to 230.\n",
    "\n",
    "- For reviews with length less than 230, we will input zeros to the beginning of the sequence. \n",
    "- For the ones that are longer than 230, we will truncate them from the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding is done.\n"
     ]
    }
   ],
   "source": [
    "max_review_len = 230\n",
    "train_pad = pad_sequences(train_seq, maxlen=max_review_len, padding='pre', truncating='pre')\n",
    "print(\"Padding is done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First review in words:\n",
      " the classic war of the worlds by timothy hines is a very entertaining film that obviously goes to great effort and lengths to faithfully recreate h g wells classic book mr hines succeeds in doing so i and those who watched his film with me appreciated the fact that it was not the standard predictable hollywood fare that comes out every year e g the spielberg version with tom cruise that had only the slightest resemblance to the book obviously everyone looks for different things in a movie those who envision themselves as amateur critics look only to criticize everything they can others rate a movie on more important baseslike being entertained which is why most people never agree with the critics we enjoyed the effort mr hines put into being faithful to h g wells classic novel and we found it to be very entertaining this made it easy to overlook what the critics perceive to be its shortcomings \n",
      "\n",
      "Same review in index: (Length: 160)\n",
      " [1, 349, 320, 4, 1, 1903, 31, 3739, 7342, 6, 3, 51, 439, 18, 11, 535, 264, 5, 83, 774, 2, 11273, 5, 12191, 9074, 2269, 1364, 3740, 349, 271, 440, 7342, 2868, 7, 395, 34, 9, 2, 143, 36, 290, 23, 18, 15, 68, 2517, 1, 185, 11, 8, 12, 20, 1, 1258, 722, 357, 2382, 11, 259, 42, 168, 284, 993, 1364, 1, 3664, 307, 15, 819, 3769, 11, 66, 60, 1, 3637, 4080, 5, 1, 271, 535, 310, 267, 14, 269, 177, 7, 3, 16, 143, 36, 20364, 526, 13, 2338, 1370, 161, 60, 5, 7058, 280, 33, 67, 378, 957, 3, 16, 19, 49, 669, 49611, 107, 2164, 59, 6, 133, 87, 80, 110, 1033, 15, 1, 1370, 71, 505, 1, 774, 440, 7342, 270, 79, 107, 2717, 5, 2269, 1364, 3740, 349, 664, 2, 71, 250, 8, 5, 25, 51, 439, 10, 89, 8, 769, 5, 4845, 47, 1, 1370, 9227, 5, 25, 28, 5667] \n",
      " \n",
      "Same review after padding:(Length: 230)\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     1   349\n",
      "   320     4     1  1903    31  3739  7342     6     3    51   439    18\n",
      "    11   535   264     5    83   774     2 11273     5 12191  9074  2269\n",
      "  1364  3740   349   271   440  7342  2868     7   395    34     9     2\n",
      "   143    36   290    23    18    15    68  2517     1   185    11     8\n",
      "    12    20     1  1258   722   357  2382    11   259    42   168   284\n",
      "   993  1364     1  3664   307    15   819  3769    11    66    60     1\n",
      "  3637  4080     5     1   271   535   310   267    14   269   177     7\n",
      "     3    16   143    36 20364   526    13  2338  1370   161    60     5\n",
      "  7058   280    33    67   378   957     3    16    19    49   669 49611\n",
      "   107  2164    59     6   133    87    80   110  1033    15     1  1370\n",
      "    71   505     1   774   440  7342   270    79   107  2717     5  2269\n",
      "  1364  3740   349   664     2    71   250     8     5    25    51   439\n",
      "    10    89     8   769     5  4845    47     1  1370  9227     5    25\n",
      "    28  5667]\n"
     ]
    }
   ],
   "source": [
    "print(\"First review in words:\\n {}\\n\".format(train_clean[1]))\n",
    "print(\"Same review in index: (Length: {1})\\n {0} \\n \".format(train_seq[1], len(train_seq[1])))\n",
    "print(\"Same review after padding:(Length: {1})\\n {0}\".format(train_pad[1], len(train_pad[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting data\n",
    "\n",
    "Original training clean data will be splitted into training set, validation set and test set because the unlabled test data is not suitable for testing our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_test: (1984, 230)\n",
      "Shape of y_test: (1984,)\n"
     ]
    }
   ],
   "source": [
    "x_test = train_pad[0:1984]\n",
    "y_test = train.sentiment[0:1984]\n",
    "\n",
    "print(\"Shape of x_test: \"+ str(x_test.shape))\n",
    "print(\"Shape of y_test: \"+ str(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of x_train: (19563, 230)\n",
      "Length of y_train: (19563,)\n",
      "\n",
      "Shape of x_valid: (3453, 230)\n",
      "Shape of y_valid: (3453,)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(train_pad[1984:], \n",
    "                                                    train.sentiment[1984:], \n",
    "                                                    test_size = 0.15, \n",
    "                                                    random_state = 123)\n",
    "\n",
    "print(\"Length of x_train: \"+ str(x_train.shape))\n",
    "print(\"Length of y_train: \"+ str(y_train.shape) +\"\\n\")\n",
    "print(\"Shape of x_valid: \"+ str(x_valid.shape))\n",
    "print(\"Shape of y_valid: \"+ str(y_valid.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batching training data\n",
    "\n",
    "In this part, we will define function to split our traing data into batches at specific length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mini_batch(x, y, batch_size):\n",
    "    total_review = x.shape[0]\n",
    "    mini_batches = []\n",
    "    number_of_batches = total_review//batch_size\n",
    "    for i in range(0, number_of_batches):\n",
    "        mini_x = x[(i*batch_size):(i+1)*batch_size, :]\n",
    "        mini_y = y[(i*batch_size):(i+1)*batch_size]\n",
    "        mini_batch = (mini_x, mini_y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train batches: 305\n",
      "\n",
      "Number of validation batches: 53\n",
      "\n",
      "Number of test batches: 31\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "mini_batches_train = mini_batch(x_train, y_train, batch_size)\n",
    "mini_batches_valid = mini_batch(x_valid, y_valid, batch_size)\n",
    "mini_batches_test = mini_batch(x_test, y_test, batch_size)\n",
    "\n",
    "print(\"Number of train batches: {}\\n\".format(len(mini_batches_train)))\n",
    "print(\"Number of validation batches: {}\\n\".format(len(mini_batches_valid)))\n",
    "print(\"Number of test batches: {}\\n\".format(len(mini_batches_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches: 305\n",
      "\n",
      "First x mini batch: (Shape: (64, 230)) \n",
      " [[ 250    5   25 ...,   24   73 2535]\n",
      " [4074   34 2698 ...,  102   10 6502]\n",
      " [  46   22 3420 ..., 2521  181  342]\n",
      " ..., \n",
      " [   0    0    0 ...,   93    5  135]\n",
      " [   0    0    0 ..., 4558   63    8]\n",
      " [1760   42    3 ...,  715   64 1132]]\n",
      "\n",
      "First y mini batch: (Shape: (64,)) \n",
      " 20853    0\n",
      "7477     0\n",
      "2625     1\n",
      "9229     1\n",
      "19569    0\n",
      "8775     0\n",
      "8823     0\n",
      "10450    0\n",
      "9891     1\n",
      "23397    1\n",
      "7222     1\n",
      "20602    0\n",
      "10350    0\n",
      "5389     1\n",
      "4245     0\n",
      "19551    1\n",
      "11211    1\n",
      "19012    0\n",
      "23303    1\n",
      "19813    0\n",
      "16801    1\n",
      "17271    0\n",
      "5906     1\n",
      "11954    1\n",
      "11219    0\n",
      "13706    0\n",
      "17989    1\n",
      "6105     0\n",
      "15069    1\n",
      "15355    0\n",
      "        ..\n",
      "7736     0\n",
      "17919    0\n",
      "3350     1\n",
      "13427    0\n",
      "13805    0\n",
      "13128    1\n",
      "15747    0\n",
      "19887    1\n",
      "17672    0\n",
      "18878    1\n",
      "8849     0\n",
      "21266    1\n",
      "5951     1\n",
      "8290     1\n",
      "10541    0\n",
      "4500     1\n",
      "12931    0\n",
      "10570    1\n",
      "3807     1\n",
      "13337    0\n",
      "9684     1\n",
      "18356    0\n",
      "9986     1\n",
      "20156    0\n",
      "5968     0\n",
      "23150    1\n",
      "3340     1\n",
      "7786     1\n",
      "2547     1\n",
      "23757    1\n",
      "Name: sentiment, Length: 64, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of batches: {}\\n\".format(len(mini_batches_train)))\n",
    "print(\"First x mini batch: (Shape: {1}) \\n {0}\\n\".format(mini_batches_train[0][0], mini_batches_train[0][0].shape))\n",
    "print(\"First y mini batch: (Shape: {1}) \\n {0}\".format(mini_batches_train[0][1], mini_batches_train[0][1].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Building RNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the standard neural network, each hidden unit only receives one input from its previous input layer. This means that information from previous steps can't be kept along the network. \n",
    "![](./images/f2.png)\n",
    "\n",
    "However, for sentiment analysis problem, the order of words in the sentences is very important to us. Therefore, we need a model that takes into account the sequence of words and RNN is a good architecture for this problem.\n",
    "In RNN, hidden unit does not only get input from its input layer but also from hidden layer of previous time step.\n",
    "\n",
    "![](./images/f3.png)\n",
    "\n",
    "Since training multilayer RNN is very expensive, in this problem, we only consider single layer RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow Graph & Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-trained Embeddings\n",
    "\n",
    "We will use the pre-trained GloVe embedding matrix taken from `https://nlp.stanford.edu/projects/glove/`. The embedding matrix is trained from Wikipedia 2014 + Gigaword 5 (6B tokens, 400K vocab) with file name `glove.6B.zip`. \n",
    "\n",
    "Inside the zip file we can find various versions of the embedding matrix which are trained with different dimensions: 50d, 100d, 200d, & 300d vectors. Here we will take the 50d version. Higher dimension will give us more information for the words but also lead to expensive computation time when training the model.\n",
    "\n",
    "The pre-trained GloVe embeddings file is a text file where every line contains the word followed by a set of vectors (here 50 values vector). We will read line by line and use a dictionary to save the data in the form of `'word':vector`. As a result, our dictionary will have 400,000 lines (= number of words of the embeddings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of word vectors: 400000\n"
     ]
    }
   ],
   "source": [
    "pretrained_embed = {} # dict to hold words and vectors from file\n",
    "pretrained_file = \"./data/glove.6B.50d.txt\"\n",
    "file = open(pretrained_file, 'r')\n",
    "for line in file:\n",
    "    split = line.split()\n",
    "    words = split[0]\n",
    "    vectors = np.asarray(split[1:],dtype = 'float32') # use np.asarray to modify the array value directly\n",
    "    pretrained_embed[words] = vectors\n",
    "file.close()\n",
    "\n",
    "print(\"Number of word vectors: {}\".format(len(pretrained_embed)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check vector values of random word.\n",
    "\n",
    "As we can see here, 50 is the dimension of the word vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vector values: (50,)\n",
      "\n",
      "Vector values for word: \n",
      "[  4.18000013e-01   2.49679998e-01  -4.12420005e-01   1.21699996e-01\n",
      "   3.45270008e-01  -4.44569997e-02  -4.96879995e-01  -1.78619996e-01\n",
      "  -6.60229998e-04  -6.56599998e-01   2.78430015e-01  -1.47670001e-01\n",
      "  -5.56770027e-01   1.46579996e-01  -9.50950012e-03   1.16579998e-02\n",
      "   1.02040000e-01  -1.27920002e-01  -8.44299972e-01  -1.21809997e-01\n",
      "  -1.68009996e-02  -3.32789987e-01  -1.55200005e-01  -2.31309995e-01\n",
      "  -1.91809997e-01  -1.88230002e+00  -7.67459989e-01   9.90509987e-02\n",
      "  -4.21249986e-01  -1.95260003e-01   4.00710011e+00  -1.85939997e-01\n",
      "  -5.22870004e-01  -3.16810012e-01   5.92130003e-04   7.44489999e-03\n",
      "   1.77780002e-01  -1.58969998e-01   1.20409997e-02  -5.42230010e-02\n",
      "  -2.98709989e-01  -1.57490000e-01  -3.47579986e-01  -4.56370004e-02\n",
      "  -4.42510009e-01   1.87849998e-01   2.78489990e-03  -1.84110001e-01\n",
      "  -1.15139998e-01  -7.85809994e-01]\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of vector values: {}\\n\".format(pretrained_embed.get(\"the\").shape))\n",
    "print(\"Vector values for word: \\n{}\".format(pretrained_embed.get(\"the\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words:            88096\n",
      "Size of embedding vectors: (88097, 50)\n"
     ]
    }
   ],
   "source": [
    "num_words = len(word_index)\n",
    "embed_size = 50 # original dimension\n",
    "word_vector = np.zeros((num_words+1, embed_size)) # +1 for padding value at 0 index\n",
    "\n",
    "print(\"Number of words:            {}\".format(num_words))\n",
    "print(\"Size of embedding vectors: {}\".format(word_vector.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are using dictionary to hold values of pre-trained embedding information, the index of dict will start at 1. As result, when looping through this dict and adding values to `word_vector` (which starts at index `0`) we use the `0` index with `0` vector values for our padding values. That's why we add 1 more when creating `word_vector`. This won't have any effect on our original vectors in dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word vector is created.\n"
     ]
    }
   ],
   "source": [
    "for key, i in word_index.items():\n",
    "    vector = pretrained_embed.get(key)\n",
    "    if vector is not None:\n",
    "        word_vector[i] = vector\n",
    "print(\"Word vector is created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embedding matrix: (88097, 50)\n",
      "\n",
      "First index in matrix are zero values (padding position): \n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of embedding matrix: {}\\n\".format(word_vector.shape))\n",
    "print(\"First index in matrix are zero values (padding position): \\n{}\".format(word_vector[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the vector of word 'the' in the original pre-trained embedding\n",
    "vector_the = np.load('./data/vector_the.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of word 'the' in our tokenized word index: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Index of word 'the' in our tokenized word index: {}\".format(word_index.get('the')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First review in words:\n",
      " the classic war of the worlds by timothy hines is a very entertaining film that obviously goes to great effort and lengths to faithfully recreate h g wells classic book mr hines succeeds in doing so i and those who watched his film with me appreciated the fact that it was not the standard predictable hollywood fare that comes out every year e g the spielberg version with tom cruise that had only the slightest resemblance to the book obviously everyone looks for different things in a movie those who envision themselves as amateur critics look only to criticize everything they can others rate a movie on more important baseslike being entertained which is why most people never agree with the critics we enjoyed the effort mr hines put into being faithful to h g wells classic novel and we found it to be very entertaining this made it easy to overlook what the critics perceive to be its shortcomings \n",
      "\n",
      "Word 'the' is at index 1 in our tokenized word list: \n",
      "[1, 349, 320, 4, 1, 1903, 31, 3739, 7342, 6, 3, 51, 439, 18, 11, 535, 264, 5, 83, 774, 2, 11273, 5, 12191, 9074, 2269, 1364, 3740, 349, 271, 440, 7342, 2868, 7, 395, 34, 9, 2, 143, 36, 290, 23, 18, 15, 68, 2517, 1, 185, 11, 8, 12, 20, 1, 1258, 722, 357, 2382, 11, 259, 42, 168, 284, 993, 1364, 1, 3664, 307, 15, 819, 3769, 11, 66, 60, 1, 3637, 4080, 5, 1, 271, 535, 310, 267, 14, 269, 177, 7, 3, 16, 143, 36, 20364, 526, 13, 2338, 1370, 161, 60, 5, 7058, 280, 33, 67, 378, 957, 3, 16, 19, 49, 669, 49611, 107, 2164, 59, 6, 133, 87, 80, 110, 1033, 15, 1, 1370, 71, 505, 1, 774, 440, 7342, 270, 79, 107, 2717, 5, 2269, 1364, 3740, 349, 664, 2, 71, 250, 8, 5, 25, 51, 439, 10, 89, 8, 769, 5, 4845, 47, 1, 1370, 9227, 5, 25, 28, 5667]\n",
      "\n",
      "Same review after padding:(Length: 230)\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     1   349\n",
      "   320     4     1  1903    31  3739  7342     6     3    51   439    18\n",
      "    11   535   264     5    83   774     2 11273     5 12191  9074  2269\n",
      "  1364  3740   349   271   440  7342  2868     7   395    34     9     2\n",
      "   143    36   290    23    18    15    68  2517     1   185    11     8\n",
      "    12    20     1  1258   722   357  2382    11   259    42   168   284\n",
      "   993  1364     1  3664   307    15   819  3769    11    66    60     1\n",
      "  3637  4080     5     1   271   535   310   267    14   269   177     7\n",
      "     3    16   143    36 20364   526    13  2338  1370   161    60     5\n",
      "  7058   280    33    67   378   957     3    16    19    49   669 49611\n",
      "   107  2164    59     6   133    87    80   110  1033    15     1  1370\n",
      "    71   505     1   774   440  7342   270    79   107  2717     5  2269\n",
      "  1364  3740   349   664     2    71   250     8     5    25    51   439\n",
      "    10    89     8   769     5  4845    47     1  1370  9227     5    25\n",
      "    28  5667]\n",
      "\n",
      "Subtract the 2 vectors to see if they are the same or not. A zero vector is our expected result.\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print(\"First review in words:\\n {}\\n\".format(train_clean[1]))\n",
    "print(\"Word 'the' is at index 1 in our tokenized word list: \\n{0}\\n\".format(train_seq[1]))\n",
    "print(\"Same review after padding:(Length: {1})\\n {0}\\n\".format(train_pad[1], len(train_pad[1])))\n",
    "# To check whether the word 'the' at 1st position in tokenized word list  \n",
    "# is corresponding to vector at 1st position in our embedding matrix\n",
    "print(\"Subtract the 2 vectors to see if they are the same or not. A zero vector is our expected result.\")\n",
    "print(word_vector[1]-vector_the)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector of word 'movie':\n",
      " [ 0.30824     0.17223001 -0.23339     0.023105    0.28522     0.23075999\n",
      " -0.41047999 -1.00349998 -0.20720001  1.43270004 -0.80684     0.68954003\n",
      " -0.43647999  1.10689998  1.61070001 -0.31966001  0.47744     0.79395002\n",
      " -0.84373999  0.064509    0.90250999  0.78609002  0.29699001  0.76056999\n",
      "  0.433      -1.50320005 -1.64230001  0.30256     0.30770999 -0.87057\n",
      "  2.47819996 -0.025852    0.50129998 -0.38593    -0.15633     0.45522001\n",
      "  0.04901    -0.42598999 -0.86401999 -1.30760002 -0.29576001  1.20899999\n",
      " -0.3127     -0.72461998 -0.80800998  0.082667    0.26738    -0.98176998\n",
      " -0.32146999  0.99822998]\n",
      "\n",
      "Vector of word 'movie' in pre-trained embedding dict:\n",
      " [ 0.30824     0.17223001 -0.23339     0.023105    0.28522     0.23075999\n",
      " -0.41047999 -1.00349998 -0.20720001  1.43270004 -0.80684     0.68954003\n",
      " -0.43647999  1.10689998  1.61070001 -0.31966001  0.47744     0.79395002\n",
      " -0.84373999  0.064509    0.90250999  0.78609002  0.29699001  0.76056999\n",
      "  0.433      -1.50320005 -1.64230001  0.30256     0.30770999 -0.87057\n",
      "  2.47819996 -0.025852    0.50129998 -0.38593    -0.15633     0.45522001\n",
      "  0.04901    -0.42598999 -0.86401999 -1.30760002 -0.29576001  1.20899999\n",
      " -0.3127     -0.72461998 -0.80800998  0.082667    0.26738    -0.98176998\n",
      " -0.32146999  0.99822998]\n",
      "\n",
      "If learned vectors are the same, the result will be zero values:\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Vector of word 'movie':\\n {}\\n\".format(word_vector[16])) # 16 is sixteenth item in word_index dict('movie')\n",
    "print(\"Vector of word 'movie' in pre-trained embedding dict:\\n {}\\n\".format(pretrained_embed.get('movie')))\n",
    "print(\"If learned vectors are the same, the result will be zero values:\\n {}\".format(pretrained_embed.get('movie')-word_vector[16]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build TensorGraph\n",
    "\n",
    "When our embedding matrix is ready, we will take advantage from the look up function of TensorFlow to look up the vectors coresponding to the word in our sequence. Hence, giving us a word vector which will be used as input for the training process.\n",
    "\n",
    "![](./images/f1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define some values for our model first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_words = len(word_index)\n",
    "embed_size = 50\n",
    "num_layers = 1\n",
    "lstm_size = 64\n",
    "n_epochs = 80\n",
    "prob = 0.5\n",
    "seq_len = max_review_len # 230\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will build the graph `g` which will be used later to feed in data for RNN training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    # Define Placeholders\n",
    "    tf_x = tf.placeholder(dtype = tf.int32, shape = (batch_size, seq_len), name = \"tf_x\")\n",
    "    tf_y = tf.placeholder(dtype = tf.float32, shape = (batch_size), name = \"tf_y\")\n",
    "    tf_keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "    # Create Embedded layer\n",
    "    embedding = tf.nn.embedding_lookup(tf.cast(word_vector, tf.float32), tf_x, name='embedding')\n",
    "\n",
    "    # Define LSTM cells\n",
    "    drop_prob = tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.BasicLSTMCell(lstm_size), output_keep_prob=tf_keep_prob)\n",
    "    lstm_cells = tf.contrib.rnn.MultiRNNCell([drop_prob] * num_layers)\n",
    "\n",
    "    # Set Initial state\n",
    "    init_state = lstm_cells.zero_state(batch_size, tf.float32)\n",
    "    lstm_outputs, final_state = tf.nn.dynamic_rnn(lstm_cells, embedding, initial_state=init_state)\n",
    "    \n",
    "    logits = tf.squeeze(tf.layers.dense(inputs=lstm_outputs[:,-1], units = 1, activation=None, name = 'logits'))\n",
    "    y_prob = tf.nn.sigmoid(logits, name = 'probabilities')\n",
    "    \n",
    "    predictions = {'probabilities': y_prob,\n",
    "                   'labels' : tf.cast(tf.round(y_prob), tf.int32,name='labels')}\n",
    "    # Cost\n",
    "    cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels = tf_y))\n",
    "    tf.summary.scalar('cost', cost)\n",
    "    \n",
    "    # Optimizer\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    starter_learning_rate = 0.1\n",
    "    learning_rt = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                           1000, 0.96, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rt)\n",
    "    train_op = optimizer.minimize(cost, name = 'train_op')\n",
    "    \n",
    "    # Accuracy\n",
    "    correct_pred = tf.equal(tf.round(y_prob), tf_y)\n",
    "    acc = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    tf.summary.scalar('accuracy', acc)\n",
    "    \n",
    "    merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train RNN model\n",
    "\n",
    "It's time to train the model. Here we will build single layer RNN with 64 LSTM hidden units. The output activation function will be sigmoid function as we only need `1` (for positive review) or `0` (negative review).\n",
    "\n",
    "Changes in the accuracy and loss can be monitored via TensorBoard. Later when the training is done, I will display the screenshots of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/80 | Train loss: 0.6691 | Train accuracy: 0.5625\n",
      "Epoch: 1/80 | Validation loss: 0.6323 | Validation accuracy: 0.6406\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_1.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 209\n",
      "Epoch: 2/80 | Train loss: 0.6713 | Train accuracy: 0.5469\n",
      "Epoch: 2/80 | Validation loss: 0.6834 | Validation accuracy: 0.5469\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_2.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 201\n",
      "Epoch: 3/80 | Train loss: 0.6920 | Train accuracy: 0.6875\n",
      "Epoch: 3/80 | Validation loss: 0.6347 | Validation accuracy: 0.6094\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_3.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 203\n",
      "Epoch: 4/80 | Train loss: 0.6314 | Train accuracy: 0.6875\n",
      "Epoch: 4/80 | Validation loss: 0.5665 | Validation accuracy: 0.6719\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_4.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 201\n",
      "Epoch: 5/80 | Train loss: 0.5994 | Train accuracy: 0.6875\n",
      "Epoch: 5/80 | Validation loss: 0.5374 | Validation accuracy: 0.7500\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_5.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 199\n",
      "Epoch: 6/80 | Train loss: 0.5784 | Train accuracy: 0.6875\n",
      "Epoch: 6/80 | Validation loss: 0.5215 | Validation accuracy: 0.7656\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_6.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 229\n",
      "Epoch: 7/80 | Train loss: 0.5340 | Train accuracy: 0.7344\n",
      "Epoch: 7/80 | Validation loss: 0.4776 | Validation accuracy: 0.7656\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_7.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 266\n",
      "Epoch: 8/80 | Train loss: 0.5209 | Train accuracy: 0.7031\n",
      "Epoch: 8/80 | Validation loss: 0.4390 | Validation accuracy: 0.7656\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_8.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 235\n",
      "Epoch: 9/80 | Train loss: 0.5187 | Train accuracy: 0.7656\n",
      "Epoch: 9/80 | Validation loss: 0.4088 | Validation accuracy: 0.7969\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_9.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 236\n",
      "Epoch: 10/80 | Train loss: 0.4414 | Train accuracy: 0.7812\n",
      "Epoch: 10/80 | Validation loss: 0.3812 | Validation accuracy: 0.7969\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_10.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 239\n",
      "Epoch: 11/80 | Train loss: 0.4629 | Train accuracy: 0.7656\n",
      "Epoch: 11/80 | Validation loss: 0.3741 | Validation accuracy: 0.8281\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_11.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 258\n",
      "Epoch: 12/80 | Train loss: 0.4801 | Train accuracy: 0.7812\n",
      "Epoch: 12/80 | Validation loss: 0.3672 | Validation accuracy: 0.8438\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_12.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 226\n",
      "Epoch: 13/80 | Train loss: 0.4737 | Train accuracy: 0.7656\n",
      "Epoch: 13/80 | Validation loss: 0.3561 | Validation accuracy: 0.8125\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_13.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 219\n",
      "Epoch: 14/80 | Train loss: 0.4648 | Train accuracy: 0.7656\n",
      "Epoch: 14/80 | Validation loss: 0.3463 | Validation accuracy: 0.8281\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_14.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 174\n",
      "Epoch: 15/80 | Train loss: 0.4716 | Train accuracy: 0.7812\n",
      "Epoch: 15/80 | Validation loss: 0.3359 | Validation accuracy: 0.8438\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_15.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 191\n",
      "Epoch: 16/80 | Train loss: 0.4763 | Train accuracy: 0.7812\n",
      "Epoch: 16/80 | Validation loss: 0.3402 | Validation accuracy: 0.8438\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_16.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 185\n",
      "Epoch: 17/80 | Train loss: 0.4487 | Train accuracy: 0.7344\n",
      "Epoch: 17/80 | Validation loss: 0.3514 | Validation accuracy: 0.7969\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_17.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 181\n",
      "Epoch: 18/80 | Train loss: 0.5067 | Train accuracy: 0.7344\n",
      "Epoch: 18/80 | Validation loss: 0.3557 | Validation accuracy: 0.8125\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_18.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 189\n",
      "Epoch: 19/80 | Train loss: 0.4505 | Train accuracy: 0.7500\n",
      "Epoch: 19/80 | Validation loss: 0.3318 | Validation accuracy: 0.8281\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_19.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 181\n",
      "Epoch: 20/80 | Train loss: 0.4559 | Train accuracy: 0.7656\n",
      "Epoch: 20/80 | Validation loss: 0.3170 | Validation accuracy: 0.8281\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_20.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 191\n",
      "Epoch: 21/80 | Train loss: 0.4353 | Train accuracy: 0.7812\n",
      "Epoch: 21/80 | Validation loss: 0.3347 | Validation accuracy: 0.8281\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_21.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 181\n",
      "Epoch: 22/80 | Train loss: 0.4361 | Train accuracy: 0.7969\n",
      "Epoch: 22/80 | Validation loss: 0.3107 | Validation accuracy: 0.8594\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_22.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 188\n",
      "Epoch: 23/80 | Train loss: 0.4197 | Train accuracy: 0.7656\n",
      "Epoch: 23/80 | Validation loss: 0.2910 | Validation accuracy: 0.8594\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_23.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 185\n",
      "Epoch: 24/80 | Train loss: 0.4097 | Train accuracy: 0.7812\n",
      "Epoch: 24/80 | Validation loss: 0.3015 | Validation accuracy: 0.8594\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_24.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 183\n",
      "Epoch: 25/80 | Train loss: 0.4484 | Train accuracy: 0.7344\n",
      "Epoch: 25/80 | Validation loss: 0.3246 | Validation accuracy: 0.8438\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_25.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 187\n",
      "Epoch: 26/80 | Train loss: 0.3688 | Train accuracy: 0.7812\n",
      "Epoch: 26/80 | Validation loss: 0.2837 | Validation accuracy: 0.8594\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_26.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 186\n",
      "Epoch: 27/80 | Train loss: 0.4113 | Train accuracy: 0.7969\n",
      "Epoch: 27/80 | Validation loss: 0.3031 | Validation accuracy: 0.8906\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_27.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 180\n",
      "Epoch: 28/80 | Train loss: 0.4128 | Train accuracy: 0.7500\n",
      "Epoch: 28/80 | Validation loss: 0.2972 | Validation accuracy: 0.8750\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_28.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 194\n",
      "Epoch: 29/80 | Train loss: 0.4386 | Train accuracy: 0.7656\n",
      "Epoch: 29/80 | Validation loss: 0.2932 | Validation accuracy: 0.8906\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_29.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 181\n",
      "Epoch: 30/80 | Train loss: 0.4669 | Train accuracy: 0.7969\n",
      "Epoch: 30/80 | Validation loss: 0.3041 | Validation accuracy: 0.8750\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_30.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31/80 | Train loss: 0.3962 | Train accuracy: 0.7969\n",
      "Epoch: 31/80 | Validation loss: 0.2984 | Validation accuracy: 0.8750\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_31.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 179\n",
      "Epoch: 32/80 | Train loss: 0.3834 | Train accuracy: 0.7812\n",
      "Epoch: 32/80 | Validation loss: 0.2954 | Validation accuracy: 0.8750\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_32.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 191\n",
      "Epoch: 33/80 | Train loss: 0.3839 | Train accuracy: 0.8281\n",
      "Epoch: 33/80 | Validation loss: 0.2620 | Validation accuracy: 0.8906\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_33.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 179\n",
      "Epoch: 34/80 | Train loss: 0.4181 | Train accuracy: 0.7969\n",
      "Epoch: 34/80 | Validation loss: 0.2852 | Validation accuracy: 0.8750\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_34.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 184\n",
      "Epoch: 35/80 | Train loss: 0.3907 | Train accuracy: 0.7812\n",
      "Epoch: 35/80 | Validation loss: 0.2882 | Validation accuracy: 0.8438\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_35.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 191\n",
      "Epoch: 36/80 | Train loss: 0.3729 | Train accuracy: 0.8281\n",
      "Epoch: 36/80 | Validation loss: 0.2817 | Validation accuracy: 0.8750\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_36.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 180\n",
      "Epoch: 37/80 | Train loss: 0.4135 | Train accuracy: 0.8125\n",
      "Epoch: 37/80 | Validation loss: 0.2666 | Validation accuracy: 0.8750\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_37.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 188\n",
      "Epoch: 38/80 | Train loss: 0.3935 | Train accuracy: 0.7812\n",
      "Epoch: 38/80 | Validation loss: 0.2825 | Validation accuracy: 0.8438\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_38.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 180\n",
      "Epoch: 39/80 | Train loss: 0.3616 | Train accuracy: 0.8281\n",
      "Epoch: 39/80 | Validation loss: 0.2730 | Validation accuracy: 0.8594\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_39.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 194\n",
      "Epoch: 40/80 | Train loss: 0.3612 | Train accuracy: 0.8281\n",
      "Epoch: 40/80 | Validation loss: 0.2742 | Validation accuracy: 0.8594\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_40.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 179\n",
      "Epoch: 41/80 | Train loss: 0.3575 | Train accuracy: 0.8125\n",
      "Epoch: 41/80 | Validation loss: 0.2974 | Validation accuracy: 0.8438\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_41.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 188\n",
      "Epoch: 42/80 | Train loss: 0.3570 | Train accuracy: 0.8125\n",
      "Epoch: 42/80 | Validation loss: 0.2899 | Validation accuracy: 0.8594\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_42.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 186\n",
      "Epoch: 43/80 | Train loss: 0.3853 | Train accuracy: 0.8594\n",
      "Epoch: 43/80 | Validation loss: 0.2490 | Validation accuracy: 0.8750\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_43.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 193\n",
      "Epoch: 44/80 | Train loss: 0.3808 | Train accuracy: 0.8281\n",
      "Epoch: 44/80 | Validation loss: 0.2751 | Validation accuracy: 0.8594\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_44.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 182\n",
      "Epoch: 45/80 | Train loss: 0.3598 | Train accuracy: 0.8125\n",
      "Epoch: 45/80 | Validation loss: 0.2867 | Validation accuracy: 0.8594\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_45.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 186\n",
      "Epoch: 46/80 | Train loss: 0.3447 | Train accuracy: 0.8125\n",
      "Epoch: 46/80 | Validation loss: 0.2835 | Validation accuracy: 0.8750\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_46.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 184\n",
      "Epoch: 47/80 | Train loss: 0.3930 | Train accuracy: 0.7812\n",
      "Epoch: 47/80 | Validation loss: 0.2772 | Validation accuracy: 0.8594\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_47.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 185\n",
      "Epoch: 48/80 | Train loss: 0.3523 | Train accuracy: 0.8438\n",
      "Epoch: 48/80 | Validation loss: 0.3018 | Validation accuracy: 0.8594\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_48.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 195\n",
      "Epoch: 49/80 | Train loss: 0.3238 | Train accuracy: 0.8281\n",
      "Epoch: 49/80 | Validation loss: 0.2858 | Validation accuracy: 0.8594\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_49.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 184\n",
      "Epoch: 50/80 | Train loss: 0.3309 | Train accuracy: 0.8750\n",
      "Epoch: 50/80 | Validation loss: 0.2675 | Validation accuracy: 0.8281\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_50.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 198\n",
      "Epoch: 51/80 | Train loss: 0.3306 | Train accuracy: 0.8750\n",
      "Epoch: 51/80 | Validation loss: 0.2803 | Validation accuracy: 0.8594\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_51.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 183\n",
      "Epoch: 52/80 | Train loss: 0.3943 | Train accuracy: 0.8281\n",
      "Epoch: 52/80 | Validation loss: 0.2833 | Validation accuracy: 0.8594\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_52.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 183\n",
      "Epoch: 53/80 | Train loss: 0.3017 | Train accuracy: 0.8750\n",
      "Epoch: 53/80 | Validation loss: 0.3366 | Validation accuracy: 0.8438\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_53.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 185\n",
      "Epoch: 54/80 | Train loss: 0.3408 | Train accuracy: 0.8750\n",
      "Epoch: 54/80 | Validation loss: 0.3072 | Validation accuracy: 0.8750\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_54.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 206\n",
      "Epoch: 55/80 | Train loss: 0.3601 | Train accuracy: 0.8281\n",
      "Epoch: 55/80 | Validation loss: 0.3269 | Validation accuracy: 0.8438\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_55.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 181\n",
      "Epoch: 56/80 | Train loss: 0.3118 | Train accuracy: 0.9062\n",
      "Epoch: 56/80 | Validation loss: 0.2996 | Validation accuracy: 0.8438\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_56.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 180\n",
      "Epoch: 57/80 | Train loss: 0.3024 | Train accuracy: 0.8906\n",
      "Epoch: 57/80 | Validation loss: 0.3400 | Validation accuracy: 0.8438\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_57.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 206\n",
      "Epoch: 58/80 | Train loss: 0.2994 | Train accuracy: 0.9062\n",
      "Epoch: 58/80 | Validation loss: 0.3238 | Validation accuracy: 0.8594\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_58.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 181\n",
      "Epoch: 59/80 | Train loss: 0.3050 | Train accuracy: 0.9219\n",
      "Epoch: 59/80 | Validation loss: 0.3499 | Validation accuracy: 0.8750\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_59.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 179\n",
      "Epoch: 60/80 | Train loss: 0.3443 | Train accuracy: 0.8281\n",
      "Epoch: 60/80 | Validation loss: 0.3452 | Validation accuracy: 0.8438\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_60.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 202\n",
      "Epoch: 61/80 | Train loss: 0.2907 | Train accuracy: 0.8438\n",
      "Epoch: 61/80 | Validation loss: 0.2874 | Validation accuracy: 0.8438\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_61.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 182\n",
      "Epoch: 62/80 | Train loss: 0.3111 | Train accuracy: 0.8906\n",
      "Epoch: 62/80 | Validation loss: 0.3471 | Validation accuracy: 0.8750\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_62.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 186\n",
      "Epoch: 63/80 | Train loss: 0.3164 | Train accuracy: 0.8438\n",
      "Epoch: 63/80 | Validation loss: 0.3628 | Validation accuracy: 0.8281\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_63.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 211\n",
      "Epoch: 64/80 | Train loss: 0.3102 | Train accuracy: 0.8438\n",
      "Epoch: 64/80 | Validation loss: 0.3410 | Validation accuracy: 0.8594\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_64.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 181\n",
      "Epoch: 65/80 | Train loss: 0.2797 | Train accuracy: 0.8750\n",
      "Epoch: 65/80 | Validation loss: 0.3154 | Validation accuracy: 0.8438\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_65.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 195\n",
      "Epoch: 66/80 | Train loss: 0.3135 | Train accuracy: 0.8125\n",
      "Epoch: 66/80 | Validation loss: 0.4264 | Validation accuracy: 0.8281\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_66.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 220\n",
      "Epoch: 67/80 | Train loss: 0.2795 | Train accuracy: 0.8594\n",
      "Epoch: 67/80 | Validation loss: 0.4376 | Validation accuracy: 0.8125\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_67.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 193\n",
      "Epoch: 68/80 | Train loss: 0.2857 | Train accuracy: 0.8906\n",
      "Epoch: 68/80 | Validation loss: 0.3647 | Validation accuracy: 0.8438\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_68.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 183\n",
      "Epoch: 69/80 | Train loss: 0.2889 | Train accuracy: 0.8594\n",
      "Epoch: 69/80 | Validation loss: 0.2799 | Validation accuracy: 0.8594\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_69.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 192\n",
      "Epoch: 70/80 | Train loss: 0.2658 | Train accuracy: 0.8906\n",
      "Epoch: 70/80 | Validation loss: 0.3159 | Validation accuracy: 0.8594\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_70.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 191\n",
      "Epoch: 71/80 | Train loss: 0.3223 | Train accuracy: 0.8281\n",
      "Epoch: 71/80 | Validation loss: 0.2854 | Validation accuracy: 0.8750\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_71.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 181\n",
      "Epoch: 72/80 | Train loss: 0.2213 | Train accuracy: 0.9062\n",
      "Epoch: 72/80 | Validation loss: 0.3015 | Validation accuracy: 0.9062\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_72.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 183\n",
      "Epoch: 73/80 | Train loss: 0.2832 | Train accuracy: 0.8594\n",
      "Epoch: 73/80 | Validation loss: 0.3390 | Validation accuracy: 0.8594\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_73.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 192\n",
      "Epoch: 74/80 | Train loss: 0.2595 | Train accuracy: 0.8906\n",
      "Epoch: 74/80 | Validation loss: 0.2721 | Validation accuracy: 0.9062\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_74.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 176\n",
      "Epoch: 75/80 | Train loss: 0.2431 | Train accuracy: 0.9219\n",
      "Epoch: 75/80 | Validation loss: 0.3459 | Validation accuracy: 0.8750\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_75.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 176\n",
      "Epoch: 76/80 | Train loss: 0.2696 | Train accuracy: 0.8750\n",
      "Epoch: 76/80 | Validation loss: 0.3748 | Validation accuracy: 0.8438\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_76.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 197\n",
      "Epoch: 77/80 | Train loss: 0.3389 | Train accuracy: 0.8594\n",
      "Epoch: 77/80 | Validation loss: 0.2765 | Validation accuracy: 0.8750\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_77.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 177\n",
      "Epoch: 78/80 | Train loss: 0.1955 | Train accuracy: 0.9062\n",
      "Epoch: 78/80 | Validation loss: 0.3131 | Validation accuracy: 0.8750\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_78.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 176\n",
      "Epoch: 79/80 | Train loss: 0.2422 | Train accuracy: 0.9062\n",
      "Epoch: 79/80 | Validation loss: 0.4281 | Validation accuracy: 0.8281\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_79.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 203\n",
      "Epoch: 80/80 | Train loss: 0.2638 | Train accuracy: 0.8906\n",
      "Epoch: 80/80 | Validation loss: 0.3829 | Validation accuracy: 0.8281\n",
      "INFO:tensorflow:./model/glove_pretrained/glove_review_sentiment_epoch_80.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 197\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph = g) as sess:\n",
    "    saver = tf.train.Saver(max_to_keep=None)\n",
    "    dt = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    logdir = \"tensorboard/\" + dt + \"/\"\n",
    "    writer = tf.summary.FileWriter(logdir, g)\n",
    "    writer_valid = tf.summary.FileWriter('tensorboard/'+ dt +'_valid', g)\n",
    "    writer_train = tf.summary.FileWriter('tensorboard/'+ dt +'_train', g)\n",
    "    iteration = 1\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        tic = datetime.now()\n",
    "        # Running train data\n",
    "        state = sess.run(init_state)\n",
    "        for batch_x_train, batch_y_train in mini_batches_train:\n",
    "            summary, c, _, state, a = sess.run([merged, cost, train_op, final_state, acc],\n",
    "                                     feed_dict = {'tf_x:0' : batch_x_train,\n",
    "                                                 'tf_y:0' : batch_y_train,\n",
    "                                                 init_state : state,\n",
    "                                                 tf_keep_prob : prob})\n",
    "\n",
    "            writer.add_summary(summary, iteration)\n",
    "            iteration +=1\n",
    "        writer_train.add_summary(summary, epoch+1)\n",
    "        print(\"Epoch: {0}/{1} | Train loss: {2:.4f} | Train accuracy: {3:.4f}\".format(epoch+1, \n",
    "                                                                                      n_epochs, \n",
    "                                                                                      c, \n",
    "                                                                                      a))\n",
    "        # Running validation data\n",
    "        valid_state = sess.run(init_state)\n",
    "        for batch_x_valid, batch_y_valid in mini_batches_valid:\n",
    "            summary, c_valid, valid_state, a_valid = sess.run([merged, cost, final_state, acc],\n",
    "                                     feed_dict = {'tf_x:0' : batch_x_valid,\n",
    "                                                 'tf_y:0' : batch_y_valid,\n",
    "                                                 init_state : valid_state,\n",
    "                                                 tf_keep_prob : 1})\n",
    "        writer_valid.add_summary(summary, epoch+1)\n",
    "        print(\"Epoch: {0}/{1} | Validation loss: {2:.4f} | Validation accuracy: {3:.4f}\".format(epoch+1, \n",
    "                                                                                                n_epochs, \n",
    "                                                                                                c_valid, \n",
    "                                                                                                a_valid))\n",
    "        # Save model every epoch\n",
    "        saver.save(sess,\"./model/glove_pretrained/glove_review_sentiment_epoch_{}.ckpt\".format(epoch+1))\n",
    "        \n",
    "        toc = datetime.now()\n",
    "        time = (toc - tic)\n",
    "        print(\"Time: {}\".format(time.seconds))\n",
    "writer.close()\n",
    "writer_train.close()\n",
    "writer_valid.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training accuracy and loss via TensorBoard\n",
    "\n",
    "TensorBoard for Accuracy and the Loss during the training time after 80 epochs. \n",
    "\n",
    "\n",
    "![](./images/acc_glove.png)\n",
    "\n",
    "\n",
    "![](./images/cost_glove.png)\n",
    "\n",
    "Below is our graph for detecting over-fitting during training. Here, after epoch 56, our model began to over-fit data. Therefore, we will pick model 56 as our last model.\n",
    "\n",
    "- <span style=\"color:rgb(70,173,193)\">Blue line </span>: training cost\n",
    "- <span style=\"color:rgb(173,73,190)\">Purple line</span>: validation cost\n",
    "\n",
    "![](./images/cost_trvd_glove.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with our test dataset\n",
    "\n",
    "Let's use our test set to test the trained model and see what is the accuracy rate. Same with the training, we also create batches for test set and run several iterations through it and keep track of accuracy in every iteration and the average of them.\n",
    "\n",
    "Below is the number of test batches we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mini_batches_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's pick our 56th model and restore it in TensorFlow session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 83.6694\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph = g) as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    accuracy = []\n",
    "    saver.restore(sess, './data/model/glove_pretrained/glove_review_sentiment_epoch_56.ckpt')\n",
    "    test_state = sess.run(init_state)\n",
    "    for batch_x, batch_y in mini_batches_test:\n",
    "        feed = {'tf_x:0': batch_x, \n",
    "            'tf_y:0': batch_y,\n",
    "            'keep_prob:0' : 1, \n",
    "            init_state : test_state}\n",
    "        a, test_state = sess.run([acc, final_state], feed_dict=feed)\n",
    "        accuracy.append(a)\n",
    "    print(\"Overall accuracy: {:.4f}\".format(np.mean(accuracy)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model with our own review\n",
    "\n",
    "First, let's create a function that can be used to transform your own written review into correct format that can be fed into tensor graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_user_review(user_review, max_seq_len, batch_size, word_index):\n",
    "    '''\n",
    "    Arguments:\n",
    "        user_review: your own input review\n",
    "        max_seq_len: this is our maximum review length that has been set to 230 in our graph, \n",
    "                     any thing longer than this will be cut.\n",
    "        word_index: words dictionary containing index for words, its index is corresponding\n",
    "                    to index in the embedding matrix\n",
    "    \n",
    "    Return:\n",
    "        Padding review with the shape of [batch_size, max_seq_len]\n",
    "        Only the last row in this array has value, the rest is zeros as we only have 1 review in this batch.\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    count = 0\n",
    "    rvw = np.zeros([batch_size, max_seq_len], dtype='int32')\n",
    "    \n",
    "    review_clean = clean_sentence(user_review)\n",
    "    review_split = review_clean.split()\n",
    "    \n",
    "    if len(review_split) > max_seq_len:\n",
    "        review_max_len = review_split[-max_seq_len:]\n",
    "    else: \n",
    "        review_max_len = review_split\n",
    "\n",
    "    len_rev = len(review_max_len)\n",
    "    temp_rvw = np.zeros(len_rev, dtype = 'int32')\n",
    "    for word in review_max_len:\n",
    "        index = word_index.get(word)\n",
    "        if index is not None:\n",
    "            temp_rvw[count] = index\n",
    "        count += 1\n",
    "    \n",
    "    review_pad = pad_sequences([temp_rvw], maxlen = max_seq_len,\n",
    "                               padding='pre', \n",
    "                               truncating='pre')\n",
    "    rvw[batch_size-1] = review_pad\n",
    "    return rvw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a negative and positive review and try them with our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "str_review_neg = \"Content is very boring and this is a waste of time to see it.\"\n",
    "str_review_pos = \"Movie is about a spy, which is not new subject. Content is very good and this is a great time to see it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will format the review to feed in our tensor graph. Here, `230` is our max sequence length, `64` is the size batch and `word_index` is our dictionary created at the beginning of the tokenizing process. We already saved this dictionary, let's load it again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pkl_file = open(\"./data/word_index_glove.pkl\", \"rb\")\n",
    "load_word_index = pickle.load(pkl_file)\n",
    "pkl_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our dictionary by looking up the index of word `it`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_word_index.get('it')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will format user review into format that can be fed into TensorGraph. Please also note that only the last row in the output array has indices of our review, 63 other rows will have zeros. The reason for this is because our graph accept input in the form [64, 230] but here we only have 1 review to feed in, so we have to create a batch of size 64 which contains 63 rows zeros. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted Review: [[ 0  0  0 ...,  0  0  0]\n",
      " [ 0  0  0 ...,  0  0  0]\n",
      " [ 0  0  0 ...,  0  0  0]\n",
      " ..., \n",
      " [ 0  0  0 ...,  0  0  0]\n",
      " [ 0  0  0 ...,  0  0  0]\n",
      " [ 0  0  0 ...,  5 63  8]]\n",
      "\n",
      "Shape of review: (64, 230)\n"
     ]
    }
   ],
   "source": [
    "user_review_neg = format_user_review(str_review_neg, 230, 64, load_word_index)\n",
    "print(\"Formatted Review: {}\\n\".format(user_review_neg))\n",
    "print(\"Shape of review: {}\".format(user_review_neg.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positive review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted Review: [[ 0  0  0 ...,  0  0  0]\n",
      " [ 0  0  0 ...,  0  0  0]\n",
      " [ 0  0  0 ...,  0  0  0]\n",
      " ..., \n",
      " [ 0  0  0 ...,  0  0  0]\n",
      " [ 0  0  0 ...,  0  0  0]\n",
      " [ 0  0  0 ...,  5 63  8]]\n",
      "\n",
      "Shape of review: (64, 230)\n"
     ]
    }
   ],
   "source": [
    "user_review_pos = format_user_review(str_review_pos, 230, 64, load_word_index)\n",
    "print(\"Formatted Review: {}\\n\".format(user_review_pos))\n",
    "print(\"Shape of review: {}\".format(user_review_pos.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running this code, we have to make sure that our TensorGraph <span style=\"color:red\">was already created</span>. \n",
    "Let's try with negative review first:\n",
    "- `1` is for positive\n",
    "- `0` is for negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a negative review.\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph = g) as sess:\n",
    "    saver.restore(sess, './data/model/glove_pretrained/glove_review_sentiment_epoch_56.ckpt')\n",
    "    own_state = sess.run(init_state)\n",
    "    \n",
    "    feed = {'tf_x:0': user_review_neg, # user_review is the formatted review.\n",
    "            'keep_prob:0' : 1,\n",
    "            init_state : own_state}\n",
    "    \n",
    "    lbl, own_state = sess.run(['labels:0', final_state], feed_dict=feed)\n",
    "    if lbl[-1] == 0:\n",
    "        print(\"This is a negative review.\")\n",
    "    else:\n",
    "        print(\"This is a positive review.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the result with our positive review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a positive review.\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph = g) as sess:\n",
    "    saver.restore(sess, './data/model/glove_pretrained/glove_review_sentiment_epoch_56.ckpt')\n",
    "    own_state = sess.run(init_state)\n",
    "    \n",
    "    feed = {'tf_x:0': user_review_pos,\n",
    "            'keep_prob:0' : 1,\n",
    "            init_state : own_state}\n",
    "    \n",
    "    lbl, own_state = sess.run(['labels:0', final_state], feed_dict=feed)\n",
    "    if lbl[-1] == 0:\n",
    "        print(\"This is a negative review.\")\n",
    "    else:\n",
    "        print(\"This is a positive review.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the end of our notebook for RNN with pretrained GloVe embedding vectors. In the next part, we will look at the models for our own trained embedding vectors using `word2vec`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
